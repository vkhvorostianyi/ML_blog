{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовые алгоритмы “машинного обучения” и  их имплементация с помощью Python\n",
    "\n",
    "Всем привет, меня зовут Хворостяный Вячеслав. Я работаю аналитиком в компании “Ring Ukraine”, в свободное время занимаюсь изучением алгоритмов “машинного обучения”, являюсь Python энтузиастом. В этой статье я хотел бы поделится с вами примером имплементации базовых алгоритмов МЛ с помощью Python, а также провести их краткий обзор.\n",
    "\n",
    "Разработки в сфере “машинного обучения” шагнули вперед с ошеломляющей, космической скоростью.Уже сейчас можно применять сложнейшие алгоритмы не особо вникая в суть того, что происходит “под капотом”. Но даже самые сложные системы состоят из атомарных, простейших частиц, которые взаимодействуют между собой.\n",
    "\n",
    "Каждый из нас уже что-то слышал об “искусственном интеллекте”. ИИ повсюду, это не новость. Прямо сейчас пока я набираю этот текст, алгоритм подсказывает мне где я делаю ошибки, помогает мне подобрать слова, то же самое происходит когда вы набираете текст у себя на смартфоне. Алгоритмы работают на нас когда мы ищем что-то в интернете, читаем почту, смотрим видео на Youtube, ставим лайки в соцсетях. Идеи эти не новы, например старушка Линейная Регрессия, которая будет рассмотрена ниже, впервые была использована Фрэнсисом Гальтоном более 130 лет назад.\n",
    "\n",
    "В этой статье я покопаюсь в упомянутых “атомах”, попытаюсь немного развеять магическую дымку, овевающую слова “машинное обучение” и “искусственный интеллект”. \n",
    "\n",
    "\n",
    "#### Одномерная линейная регрессия\n",
    "\n",
    "\n",
    "Данный алгоритм является одним из самых широко используемых алгоритмов МЛ, а также самым прозрачным  и интерпретируемым. Скорей всего вы уже стыкались с ним в повседневной жизни, а может и применяли на практике. Линейная регрессия это алгоритм позволяющий отобразить линейную зависимость между двумя переменными, выражается формулой:\n",
    "\n",
    "$$\n",
    "h^{(i)} = \\beta_0 + \\beta_1x^{(i)} \\\\\n",
    "\\tag{1}\n",
    "$$\n",
    "где $h$ - предположение(hypothesis), $(\\beta_0, \\beta_1)^T$- вектор параметров, в котором и заключается вся магия, $x$ - независимая переменная\n",
    "\n",
    "Формула линейной регрессии — это уравнение прямой, коэффициент $\\beta_0$ задает смещение этой прямой по оси $y$, а $\\beta_1$ - угол наклона. Вся суть в том, чтобы как можно лучше подобрать параметры $\\beta_0$ и $\\beta_1$ при заданном $x$, при этом $h$ будет отображать ожидаемое значение функции.\n",
    "\n",
    "Линейная регрессия относится к так называемому \"обучению с учителем\" или \"supervised machine learning\", это значит, что\n",
    "у нас уже есть некоторое количество данных где известны как значение аргументов, так и значения самой функции $y$, наша задача состоит в том чтобы \"обучится\" на этих, уже размеченных данных, а потом применяя полученный вектор параметров к любому случайному $x$, получать соответствующий $y$.\n",
    "\n",
    "Имея $x$ и $y$ можем рассчитать интересующие нас параметры по формулам:\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n",
    "$$\n",
    "\n",
    "Где $\\bar{x}$, $\\bar{y}$ - математическое ожидание, оно же среднее арифметическое от векторов $X$ и $Y$\n",
    "\n",
    "Данные формулы являются частным случаем метода наименьших квадратов, к которому мы еще вернемся, более подробно о том как выводятся эти формулы можно узнать перейдя по ссылке https://www.wikiwand.com/en/Ordinary_least_squares, ну а если коротко, чтобы получить $\\beta_1$ нужно подсчитать все отклонения от среднего для векторов $x$ и $y$, в числителе взять суму произведений этих отклонений, а в знаменателе суму квадратов отклонений от среднего по $x$, $\\beta_0$ получаем простой подстановкой.\n",
    "\n",
    "\n",
    "Имплементация на Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "def ord_LinReg_fit(X,Y):\n",
    "    x_mean = np.mean(X)\n",
    "    y_mean = np.mean(Y)\n",
    "\n",
    "    num = np.sum((X - x_mean)*(Y - y_mean))\n",
    "    denom = np.sum((X - x_mean)**2)\n",
    "    \n",
    "    b_1 = num/denom\n",
    "    b_0 = y_mean - b_1*x_mean\n",
    "    return b_0,b_1\n",
    "\n",
    "# Допустим мы хотим изучить зависимость веса мозгов Y от объёма черепной коробки X\n",
    "X = np.array([3443, 3993, 3640, 4208, 3832, 3876, 3497, 3466, 3095, 4424]) # см^3\n",
    "Y = np.array([1340, 1380, 1355, 1522, 1208, 1405, 1358, 1292, 1340, 1400]) # граммы\n",
    "\n",
    "b0,b1 = ord_LinReg_fit(X,Y) # вычислeние параметров\n",
    "H = b0 + b1 * X\n",
    "\n",
    "# Визуализация\n",
    "plt.plot(X,H, c='b', label='Regression Line')\n",
    "plt.scatter(X, Y, c='g', label='Known values')\n",
    "```\n",
    "![image.png](files/lin_reg.png)\n",
    "\n",
    "\n",
    "#### Многомерная линейная регрессия\n",
    "\n",
    "Многомерную линейную регрессию можно рассматривать как обобщение одномерной, данной выше.\n",
    "\n",
    "\n",
    "$$\n",
    "H = \\beta_0x_0 + \\beta_1x_1 + \\beta_2x_2 + … + \\beta_nx_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_0 = 1 \n",
    "$$\n",
    "\n",
    "Единственное изменение состоит в том, что для удобства расчета к первому параметру $\\beta_0$ была добавлена еще одна независимая переменная $x_0$, равна единице.\n",
    "\n",
    "Уравнение приведенное выше можно записать в свернутой форме как:\n",
    "$$\n",
    "H = \\theta^TX\n",
    "$$\n",
    "\n",
    "Где \n",
    "$$\n",
    "\\theta = [ \\beta_0 + \\beta_1 + \\beta_2 + … + \\beta_n ] \n",
    "$$\n",
    "$$\n",
    "X = [ x_0 + x_1 + x_2 + … + x_n]\n",
    "$$\n",
    "\n",
    "\n",
    "Согласно методу наименьших квадратов, вектор параметров можно получить путем решения нормального уравнения\n",
    "$$\n",
    "\\theta = (X^{T}*X)^{-1}X^{T}Y\n",
    "$$\n",
    "где $\\theta$ - вектор параметров\n",
    "\n",
    "Имплементация на Python:\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as \n",
    "%matplotlib inline\n",
    "\n",
    "def get_X_with_ones(X):\n",
    "    \"\"\"добавляем столбик единиц\"\"\"\n",
    "    m = len(X)\n",
    "    X = np.c_[np.ones(m),X]\n",
    "    return X\n",
    "\n",
    "def LSM_fit(X,Y):\n",
    "    X_inv = np.linalg.inv(np.matmul(X.T,X))\n",
    "    middle_res = np.matmul(X_inv,X.T)\n",
    "    theta = np.matmul(middle_res,Y)\n",
    "    return theta\n",
    "\n",
    "X = np.array([3443, 3993, 3640, 4208, 3832, 3876, 3497, 3466, 3095, 4424]) # см^3\n",
    "Y = np.array([1340, 1380, 1355, 1522, 1208, 1405, 1358, 1292, 1340, 1400]) # граммы\n",
    "\n",
    "X_ = get_X_with_ones(X)\n",
    "theta = LSM_fit(X_,Y)\n",
    "H = X_.dot(theta.T)\n",
    "```\n",
    "\n",
    "У этого подхода есть несколько недостатков:\n",
    "- Не во всех случаях матрица $(X^{T}*X)^{-1}$ существует\n",
    "- При большом количестве независимых переменных этот способ требует большой вычислительной мощности\n",
    "\n",
    "Поэтому на практике гораздо чаще можно встретить другой алгоритм вычисления вектора параметров, а именно\n",
    "**градиентный спуск**.\n",
    "\n",
    "###### Градиентный спуск и кост-функция\n",
    "Градиентный спуск(gradient descent) - алгоритм оптимизации, который позволяет обновлять значения всех элементов вектора параметров одновременно.  \n",
    "Градиент - это вектор частных производных от $\\theta$.  \n",
    "Метод предполагает обновление вектора $\\theta$ по всем параметрам, за $n$ шагов.  \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{\\textrm{(i)}}) - y^{\\textrm{(i)}})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_{j}^{(i)}\n",
    "$$\n",
    "\n",
    "Где  \n",
    "$J(\\theta)$ - кост-функция(cost function),  \n",
    "$\\alpha$ - так называемый \"шаг обучения\"(learning rate), его нужно подобрать вручную, так, чтобы $J(\\theta)$ достигала минимума при как можно меньшем количестве итераций.  \n",
    "$h_\\theta(x^{(i)})$ - гипотеза  \n",
    "$y$ - истинное значение\n",
    "\n",
    "Так как из $J(\\theta) = 0$ вытекает, что гипотеза $h_\\theta(x^{(i)})$\n",
    "полностью совпадает с действительным значением $y$, задачу метода можно сформулировать как нахождение минимального значения $J(\\theta)$ при наименьшем числе итераций. Другими словами, мы подбираем новые коэффициенты с каждой итерацией, пока результат не будет достаточно близок к истинному значению, при этом $J(\\theta)$ является индикатором этого приближения.\n",
    "\n",
    "\n",
    "Метод градиентного спуска широко используется в алгоритмах \"машинного обучения\", для решения самых разнообразных задач.\n",
    "\n",
    "\n",
    "Имплементация на Python:\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_X_with_ones(X):\n",
    "    \"\"\"нормализируем данные и добавляем столбик единиц\"\"\"\n",
    "    X = np.array(X)\n",
    "    mean,std = X.mean(), X.std()\n",
    "    X = (X - mean) / std\n",
    "    m = len(X)\n",
    "    X = np.c_[np.ones(m),X].T\n",
    "    return X\n",
    "\n",
    "def propagate(theta, X, Y):\n",
    "    m = X.shape[1]\n",
    "    H = np.dot(theta.T,X)\n",
    "    cost = np.sum((H-Y)**2)/(2*m) \n",
    "    grads = np.dot(X,(H-Y).T)/m\n",
    "    return grads, cost\n",
    "\n",
    "def gradient_descent(X, Y, theta, alpha, iterations):\n",
    "    cost_hist = []\n",
    "    for i in range(iterations):\n",
    "            gradient, cost = propagate(theta, X, Y)\n",
    "            theta -= alpha*gradient\n",
    "            cost_hist.append(cost)\n",
    "    return theta, cost_hist\n",
    "\n",
    "X = np.array([3443, 3993, 3640, 4208, 3832, 3876, 3497, 3466, 3095, 4424]) # см^3\n",
    "Y = np.array([1340, 1380, 1355, 1522, 1208, 1405, 1358, 1292, 1340, 1400]) # граммы\n",
    "\n",
    "alpha = 1\n",
    "theta = np.zeros((X_.shape[0],1))\n",
    "X_ = get_X_with_ones(X)\n",
    "theta, cost_hist = gradient_descent(X_, Y, theta, alpha, 10)\n",
    "H = np.squeeze(np.dot(theta.T,X_))\n",
    "```\n",
    "\n",
    "\n",
    "#### Оценка точности модели\n",
    "\n",
    "Для того чтобы понимать, насколько хорошо или плохо модель справляется с задачей нужно каким то образом оценить результат.\n",
    "В этом нам помогут такие метрики как **корень от среднеквадратической ошибки (root mean squared error)** и **коэффициент детерминации (coefficient of determination)**.\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\sum_{i=1}^{m} \\frac{1}{m} (\\hat{y_i} - y_i)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "SS_t = \\sum_{i=1}^{m} (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "SS_r = \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "$$\n",
    "R^2 \\equiv 1 - \\frac{SS_r}{SS_t}\n",
    "$$\n",
    "\n",
    "Где\n",
    "$RMSE$ - корень от среднеквадратической ошибки(root mean squared error)\n",
    "$\\hat{y_i}$ - гипотеза\n",
    "$y_i$ - действительное значение\n",
    "$\\bar{y}$ - среднее арифметическое от действительного значения\n",
    "$R^2$ - коэффициент детерминации\n",
    "\n",
    "Чем ниже $RMSE$ и чем выше $R^2$ тем лучше модель справляется с задачей.\n",
    "Имплементация на Python:\n",
    "\n",
    "```python\n",
    "def rmse(Y, H):\n",
    "    m = len(Y)\n",
    "    mse = sum((Y- H)**2)\n",
    "    rmse = np.sqrt(mse/m)\n",
    "    return rmse\n",
    "\n",
    "def r_2(Y,H):\n",
    "    y_mean = np.mean(Y)\n",
    "    ss_t = sum((Y - y_mean)**2)\n",
    "    ss_r = sum((Y - H)**2)\n",
    "    r_2 = 1 - (ss_r/ss_t)\n",
    "    return r_2\n",
    "```\n",
    "\n",
    "#### Применение алгоритмов\n",
    "\n",
    "Теперь можно попробовать применить рассмотренные алгоритмы на практике и сравнить их с уже готовой имплементацией sklearn. Сделаем это в несколько шагов:\n",
    "\n",
    "1. Загружаем датасет и разделяем данные на тренинг-сет и тест-сет, для sklearn нужно правильно задать размерности, метод ```reshape()``` помогает справиться с этой задачей.\n",
    "\n",
    "```python \n",
    "import pandas as pd\n",
    "df = pd.read_csv('files/brainhead.csv')\n",
    "\n",
    "def split_data(df,reshape=False):\n",
    "    from sklearn.model_selection import train_test_split  \n",
    "    x = df.values[:, 2] # предпоследняя колонка - объем черепной коробки\n",
    "    y = df.values[:, 3] # вес мозга\n",
    "    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "    if reshape:\n",
    "        return (x.reshape(-1,1) for x in [train_set_x, test_set_x, train_set_y, test_set_y])\n",
    "    return train_set_x, test_set_x, train_set_y, test_set_y\n",
    "```\n",
    "2. Применяем кастомную модель \n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_X_with_ones(X):\n",
    "    \"\"\"нормализируем данные и добавляем столбик единиц\"\"\"\n",
    "    X = np.array(X)\n",
    "    mean,std = X.mean(), X.std()\n",
    "    X = (X - mean) / std\n",
    "    m = len(X)\n",
    "    X = np.c_[np.ones(m),X].T\n",
    "    return X\n",
    "\n",
    "def propagate(theta, X, Y):\n",
    "    m = X.shape[1]\n",
    "    H = np.dot(theta.T,X)\n",
    "    cost = np.sum((H-Y)**2)/(2*m) \n",
    "    grads = np.dot(X,(H-Y).T)/m\n",
    "    return grads, cost\n",
    "\n",
    "def gradient_descent(X, Y, theta, alpha, iterations):\n",
    "    cost_hist = []\n",
    "    for i in range(iterations):\n",
    "            gradient, cost = propagate(theta, X, Y)\n",
    "            theta -= alpha*gradient\n",
    "            cost_hist.append(cost)\n",
    "    return theta, cost_hist\n",
    "\n",
    "def rmse(Y, H):\n",
    "    m = len(Y)\n",
    "    mse = sum((Y- H)**2)\n",
    "    rmse = np.sqrt(mse/m)\n",
    "    return rmse\n",
    "\n",
    "def r_2(Y,H):\n",
    "    y_mean = np.mean(Y)\n",
    "    ss_t = sum((Y - y_mean)**2)\n",
    "    ss_r = sum((Y - H)**2)\n",
    "    r_2 = 1 - (ss_r/ss_t)\n",
    "    return r_2\n",
    "\n",
    "train_set_x, test_set_x, train_set_y, test_set_y = load_data(df)\n",
    "X,Y = train_set_x, train_set_y\n",
    "X_test = get_X_with_ones(test_set_x)\n",
    "\n",
    "alpha = 1\n",
    "X_ = get_X_with_ones(X)\n",
    "theta = np.zeros((X_.shape[0],1))\n",
    "theta, cost_hist = gradient_descent(X_, Y, theta, alpha, 20)\n",
    "H = np.squeeze(np.dot(theta.T,X_test))\n",
    "\n",
    "print(f'r^2: {r_2(test_set_y,H)} rmse: {rmse(test_set_y,H)}')\n",
    "\n",
    "```\n",
    "Результат:\n",
    "``` r^2: 0.672757755764332 rmse: 68.54674574377383 ```\n",
    "\n",
    "3. Применяем готовую модель с библиотеки\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_set_x, test_set_x, train_set_y, test_set_y = load_data(df, reshape=True)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(train_set_x, train_set_y)\n",
    "\n",
    "y_pred = model.predict(test_set_x)\n",
    "mse = mean_squared_error(y_pred, test_set_y) \n",
    "\n",
    "r_2_sklearn = model.score(train_set_x, train_set_y)\n",
    "rmse_sklearn = np.sqrt(mse)\n",
    "\n",
    "print(f'r^2_sklearn: {r_2_sklearn}, rmse_sklearn: {rmse_sklearn}')\n",
    "```\n",
    "\n",
    "Результат:\n",
    "``` r^2_sklearn: 0.6236128413780265, rmse_sklearn: 68.91317515113433 ```\n",
    "\n",
    "В итоге у кастомной модели немного меньше ошибка и соответственно выше коэффициент детерминации.\n",
    "\n",
    "\n",
    "В этой статье были приведены базовые алгоритмы \"машинного обучения\" и их имплементация на Python, а уже в следующей, будет рассмотрен алгоритм логистической регрессии для решения задач классификации объектов.\n",
    "\n",
    "Для написания этой статьи использовались материалы:  \n",
    "https://mubaris.com/posts/linear-regression/  \n",
    "https://www.wikiwand.com/en/Ordinary_least_squares  \n",
    "https://www.coursera.org/learn/machine-learning  \n",
    "\n",
    "Ссылка на датасеты и многое другое: https://www.kaggle.com/datasets   \n",
    "Ссылка на проект: https://github.com/vkhvorostianyi/ML_blog/tree/master/articles/linear_regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
