{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовые алгоритмы “машинного обучения” и  их имплементация с помощью Python\n",
    "\n",
    "Всем привет, меня зовут Хворостяный Вячеслав. Я работаю аналитиком в компании “Ring Ukraine”, в свободное время занимаюсь изучением алгоритмов “машинного обучения”, являюсь Python энтузиастом. В этой статье я хотел бы поделится с вами примером имплементации базовых алгоритмов МЛ с помощью Python, а также провести их краткий обзор.\n",
    "\n",
    "Разработки в сфере “машинного обучения” шагнули вперед с ошеломляющей, космической скоростью, уже сейчас можно применять сложнейшие алгоритмы не особо вникая в суть того, что происходит “под капотом”. Но даже самые сложные системы состоят из атомарных, простейших частиц, которые взаимодействуют между собой.\n",
    "\n",
    "Каждый из нас уже что-то слышал об “искусственном интеллекте”, ИИ повсюду, это не новость, прямо сейчас пока я набираю этот текст, алгоритм подсказывает мне где я делаю ошибки и помогает мне подобрать слова, тоже самое происходит когда вы набираете текст у себя на смартфоне. Алгоритмы работают на нас когда мы ищем что-то в интернете, читаем почту, смотрим видео на Youtube, ставим лайки в соцсетях. Идеи эти не новы, например старушка Линейная Регрессия, которая будет рассмотрена ниже, впервые была использована Френсисом Гальтоном более 130 лет назад.\n",
    "\n",
    "В этой статье мы попытаемся разобраться в “атомах” из которых состоят эти сложные системы, немного развеять магическую дымку, опутывающую изменения, принесенные в наш мир “машинным обучением” и “искусственным интеллектом”. \n",
    "\n",
    "\n",
    "#### Одномерная линейная регрессия\n",
    "\n",
    "\n",
    "Данный алгоритм является одним из самых широко используемых алгоритмов МЛ, а также самым прозрачным  и интерпретируемым. Скорей всего вы уже стыкались с ним в повседневной жизни, а может и применяли на практике. Линейная регрессия это алгоритм позволяющий отобразить линейную зависимость между двумя переменными, выражается формулой:\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(i)} = \\beta_0 + \\beta_1x^{(i)} \\\\\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "где $h$ - предположение(hypothesis), $(\\beta_0, \\beta_1)^T$- вектор параметров, в котором и заключается вся магия, $x$ - независимая переменная\n",
    "\n",
    "Вся суть в том, чтобы как можно лучше подобрать параметры $\\beta_0$ и $\\beta_1$ при заданном $x$, при этом $h$ будет отображать ожидаемое значение функции.\n",
    "\n",
    "Линейная регрессия относится к так називаемому \"обучению с учителем\" или \"supervised machine learning\", это значит, что \n",
    "у нас уже есть некоторое количество данных где известны как заначение аргументов функции $x$ , так и значения самой функции $y$, наша задача состоит в том чтобы \"обучится\" на этих, уже размеченых данных, а потом применяя полученый вектор параметров к любому случайному $x$, получать соответствующий $y$.\n",
    "\n",
    "Имея $x$ и $y$ мы можем расчитать интересующие нас параметры за формулами:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\n",
    "\\tag{2}\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "Где $\\bar{x}$, $\\bar{y}$- математическое ожидание от векторов $X$ и $Y$\n",
    "\n",
    "Данные формулы являются частным случаем метода наименших квадратов, к которому мы еще вернемся, более подробно об о том как выводятся эти формулы можно узнать перейдя по ссылке https://www.wikiwand.com/en/Ordinary_least_squares\n",
    "\n",
    "\n",
    "Имплементация на Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def ord_LinReg_fit(X,Y):\n",
    "    x_mean = np.mean(X)\n",
    "    y_mean = np.mean(Y)\n",
    "\n",
    "    m = len(X)\n",
    "\n",
    "    number = 0\n",
    "    denom  = 0\n",
    "    for i in range(m):\n",
    "        number += (X[i] - x_mean)*(Y[i] - y_mean)\n",
    "        denom += (X[i] - x_mean)**2\n",
    "\n",
    "    b_1 = number/denom\n",
    "    b_0 = y_mean - (b_1*x_mean)\n",
    "    return b_0,b_1\n",
    "\n",
    "# Допустим мы хотим изучить зависимость веса мозгов Y от обьема черепной коробки X\n",
    "X = [3443, 3993, 3640, 4208, 3832, 3876, 3497, 3466, 3095, 4424] # см^3\n",
    "Y = [1340, 1380, 1355, 1522, 1208, 1405, 1358, 1292, 1340, 1400] # граммы\n",
    "\n",
    "\n",
    "max_x = np.max(X) + 100\n",
    "min_x = np.min(X) - 100\n",
    "\n",
    "\n",
    "reg_line_x = np.linspace(min_x, max_x, 10) # новые значения Х\n",
    "b0,b1 = ord_LinReg_fit(X,Y) # вычислeние параметров\n",
    "H = b0 + b1 * reg_line_x\n",
    "\n",
    "# Визуализация\n",
    "plt.plot(reg_line_x, H, c='b', label='Regression Line')\n",
    "plt.scatter(X, Y, c='g', label='Known values')\n",
    "```\n",
    "![image.png](lin_reg.png)\n",
    "\n",
    "\n",
    "#### Многомерная линейная регрессия\n",
    "\n",
    "Многомерную линейную регрессию можно рассматривать как обобщение одномерной, данной выше.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "H = \\beta_0x_0 + \\beta_1x_1 + \\beta_2x_2 + … + \\beta_nx_n\n",
    "\\label{eq:eq_3}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x_0 = 1 \n",
    "\\end{equation}\n",
    "\n",
    "Единственное измение состоит в том, что для удобства расчета к первому параметру $\\beta_0$ была добавлена еще одна независимая переменная $x_0$, равна единице.\n",
    "\n",
    "Уравнение $\\eqref{eq:eq_3}$ можно записать в свернутом виде как:\n",
    "\\begin{equation}\n",
    "H = \\theta^TX\n",
    "\\label{eq:eq_4}\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Где \n",
    "\\begin{equation}\n",
    "\\theta = [ \\beta_0 + \\beta_1 + \\beta_2 + … + \\beta_n ] \\\\\n",
    "X = [ x_0 + x_1 + x_2 + … + x_n]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Согласно методу наименьших квадратов, вектор параметров можно получить путем решения нормального уравнения\n",
    "\\begin{equation}\n",
    "\\theta = (X^{T}*X)^{-1}X^{T}Y\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "где $\\theta$ - вектор параметров\n",
    "\n",
    "Но у этого подхода есть несколько недостатков"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
