{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия\n",
    "\n",
    "\n",
    "Всем привет, меня зовут Хворостяный Вячеслав. В этой статье я продолжу тему имплементации базовых алгоритмов \"машинного обучения\" с помощью Python. В предидущей статье был расмотрен алгоритм линейной регресии, а также такой метод оптимизации как гадиентный спуск. В этой статье разберем логистическую регрессию, регуляризацию и применения алгоритмов на практике.\n",
    "\n",
    "Существует два основных типа алгоритмов \"машинного обучения\" - \"обучение с учетелем\" (\"supervised learning\") и \"обучение без учителя\" (\"unsupervised learning\"). В свою очередь, \"обучение с учителем\" делиться на регрессионные алгоритмы и алгоритмы класификации. Логистическая регрессия пренадлежит к типу \"supervised learning\", и, несмотря на название, является алгоритмом класификации, часто используемым как функция активации в нейронных сетях, или как самостоятельный метод класификации объектов. \"Обучение с учителем\" значит, что у нас уже есть некоторое количество данных где известны как значение аргументов, так и значения самой функции, и наша задача состоит в том, чтобы \"обучится\" на этих, уже размеченных данных.\n",
    "\n",
    "В основе алгоритма лежит экспоненциальная кривая или сигмоида, которая разделяет объекты на два класса, в итоге получаем бинарную класификацию.\n",
    "\n",
    "$$z^{(i)} = w^T x^{(i)} + b $$\n",
    " \n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$\n",
    "\n",
    "$$sigmoid(z^{(i)}) = \\frac{1}{1-e^{-z^{(i)}}}$$\n",
    "\n",
    "\n",
    "\n",
    "Где   \n",
    "$z^{(i)}$ - функция прямой (линейная регрессия)  \n",
    "$\\hat{y}^{(i)}$ - гипотеза   \n",
    "$sigmoid(z^{(i)})$ - сигмоида \n",
    "\n",
    "Задача алгоритма - вичислить вероятность того, что елемент пренадлежит к одному из классов. Значения функции находятся в границах от [0,1], самый простой способ - округлить все значения больше 0.5 до единицы,эту границу можно поднять и выше, в том случае, если верно определить позитивный класс является задачей более критичной чем отличить красный носок от белого.\n",
    "\n",
    "Кост-функция в случае логистической регрессии будет выглядеть так:\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m - y^{(i)} \\log(a^{(i)}) -  (1-y^{(i)} ) \\log(1-a^{(i)})$$\n",
    "\n",
    "\n",
    "Датасет может иметь много полей или \"фичей\", может быть много, но не все фичи вносят одинаковый вклад в принятие алгоритмом решения. По сути признаки (фичи) с малим вкладом являются \"шумом\" для алгоритма и вносят нежелательный вклад, который називают \"переобучение\" или \"overfitting\", алгоритм \"заучивается\" на текущих данных и плохо работает с новимы. В свою очередь недостаточное количество фичей вызывает \"недообучение\" или \"underfitting\", модель плохо показывает себя как на тренинг сете, так и на тесте, сложно оценить стоимость автомобиля, зная только что у него сиденья с подогревом, согласитесь. Серебряной пули нет, и к каждой проблеме нужно подходить индивидуально, но при большом количестве фичей регуляризация спасает от обучения модели на \"шуме\". \n",
    "\n",
    "##### Регуляризация\n",
    "Регуляризация - это подход, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
